{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "SINTIA AFRIYANI(20000150346)NLP .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N1M0rwsArye"
      },
      "source": [
        "## **Pembuka**"
      ],
      "id": "6N1M0rwsArye"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nToGlpTgAryf"
      },
      "source": [
        "Assalamu'alaikum warahmatullahi wabarakatuh.\n",
        "Puji syukur kehadirat Allah Subhana Wata'ala atas limpahan Rahmat dan HidayahNya kepada kita semua.\n",
        "Sholawat serta salam senantiasa tercurah limpahkan kepada baginda Muhammad Rasulullah Salallahualaihiwassalam.\n",
        "\n",
        "Halo para **Pejuang Data**. Selamat berjumpa di pertemuan ketujuh Program Training **Algoritma Machine Learning** Kelas Mahir.\n",
        "\n",
        "Pada pertemuan ini kamu akan belajar:\n",
        "* Apa itu NLP\n",
        "* Scrapping Data\n",
        "* Text Preprocessing\n",
        "    * Case Folding & Data Cleaning\n",
        "    * Lemmatisasi\n",
        "    * Stemming\n",
        "    * Slang Word\n",
        "    * Stop Word\n",
        "    * Unwanted Word"
      ],
      "id": "nToGlpTgAryf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hH-wRupAryj"
      },
      "source": [
        "### **Import Library**"
      ],
      "id": "3hH-wRupAryj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdHTXfnSAryk"
      },
      "source": [
        "import sys\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")"
      ],
      "id": "VdHTXfnSAryk",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJa_PvN9Aryn"
      },
      "source": [
        "## **Natural Language Processing**"
      ],
      "id": "mJa_PvN9Aryn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2QLfmeVAryo"
      },
      "source": [
        "Natural Language Processing (NLP) merupakan salah satu cabang ilmu AI yang berfokus pada pengolahan bahasa natural. Bahasa natural adalah bahasa yang secara umum digunakan oleh manusia dalam berkomunikasi satu sama lain.  Bahasa yang diterima oleh komputer butuh untuk diproses dan dipahami terlebih dahulu supaya maksud dari user bisa dipahami dengan baik oleh komputer.\n",
        "\n",
        "Ada berbagai terapan aplikasi dari NLP. Diantaranya adalah Chatbot (aplikasi yang membuat user bisa seolah-olah melakukan komunikasi dengan computer), Stemming atau Lemmatization (pemotongan kata dalam bahasa tertentu menjadi bentuk dasar pengenalan fungsi setiap kata dalam kalimat), Summarization (ringkasan dari bacaan), Translation Tools (menterjemahkan bahasa) dan aplikasi-aplikasi lain yang memungkinkan komputer mampu memahami instruksi bahasa yang diinputkan oleh user.\n",
        "\n",
        "Pustejovsky dan Stubbs (2012) menjelaskan bahwa ada beberapa area utama penelitian pada field NLP, diantaranya:\n",
        "\n",
        "1. **Question Answering Systems (QAS)**. Kemampuan komputer untuk menjawab pertanyaan yang diberikan oleh user. Daripada memasukkan keyword ke dalam browser pencarian, dengan QAS, user bisa langsung bertanya dalam bahasa natural yang digunakannya, baik itu Inggris, Mandarin, ataupun Indonesia.\n",
        "\n",
        "2. **Summarization**. Pembuatan ringkasan dari sekumpulan konten dokumen atau email. Dengan menggunakan aplikasi ini, user bisa dibantu untuk mengkonversikan dokumen teks yang besar ke dalam bentuk slide presentasi.\n",
        "Machine Translation. Produk yang dihasilkan adalah aplikasi yang dapat memahami bahasa manusia dan menterjemahkannya ke dalam bahasa lain. Termasuk di dalamnya adalah Google Translate yang apabila dicermati semakin membaik dalam penterjemahan bahasa. Contoh lain lagi adalah BabelFish yang menterjemahkan bahasa pada real time.\n",
        "\n",
        "3. **Speech Recognition**. Field ini merupakan cabang ilmu NLP yang cukup sulit. Proses pembangunan model untuk digunakan telpon/komputer dalam mengenali bahasa yang diucapkan sudah banyak dikerjakan. Bahasa yang sering digunakan adalah berupa pertanyaan dan perintah.\n",
        "\n",
        "4. **Document classification**. Sedangkan aplikasi ini adalah merupakan area penelitian NLP Yang paling sukses. Pekerjaan yang dilakukan aplikasi ini adalah menentukan dimana tempat terbaik dokumen yang baru diinputkan ke dalam sistem. Hal ini sangat berguna pada aplikasi spam filtering, news article classification, dan movie review."
      ],
      "id": "S2QLfmeVAryo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk1914XF-Bkh"
      },
      "source": [
        "## **Scrapping Data Text**"
      ],
      "id": "Pk1914XF-Bkh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmidxKbiAryx"
      },
      "source": [
        "Sebelum melakukan penerapan dan berbagai penelitian. Mengumpulkan data teks sebagai bahan dasar dari bidang ini merupakan hal yang sangat penting. Proses ini biasa disebut dengan scrapping data. Aktivitas scrapping data bisa dilakukan melalui berbagai platfrom. Mulai langsung pada halaman web tertentu, melalui API seperti Twitter, atau melalui tools yang sudah disediakan, bisa free atau berbayar.  Untuk mulai belajar NLP, kita akan menggunakan tools. Toolls/Library `google_play_scrapper` adalah library yang dapat digunakan untuk mengambil review dari google apps. Pertama kita perlu melakukan instalasi sebagai berikut."
      ],
      "id": "PmidxKbiAryx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WwgeUitAryz"
      },
      "source": [
        "**Instalasi google play scrapper**"
      ],
      "id": "5WwgeUitAryz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcemcLSj8nSN",
        "outputId": "a886b630-2087-4b1a-ace1-b625d75047ac"
      },
      "source": [
        "!pip install google_play_scraper"
      ],
      "id": "OcemcLSj8nSN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google_play_scraper\n",
            "  Downloading google-play-scraper-1.0.2.tar.gz (52 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 30 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 40 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 51 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: google-play-scraper\n",
            "  Building wheel for google-play-scraper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-play-scraper: filename=google_play_scraper-1.0.2-py3-none-any.whl size=24393 sha256=e94dcf7f4cd5e83ceb93812731a55a52136994c2916284a66f02b1f878b7bef6\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/99/eb/bbb9d24a5c526980647efc10336eaaeffcf07749f581111128\n",
            "Successfully built google-play-scraper\n",
            "Installing collected packages: google-play-scraper\n",
            "Successfully installed google-play-scraper-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnso1Toc8a2z"
      },
      "source": [
        "**Import Library**"
      ],
      "id": "jnso1Toc8a2z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8LXL6D6826U"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google_play_scraper import Sort, reviews                  # Librray untuk scrapping data teks\n",
        "import re                                                      # Library untuk teks preprocessing"
      ],
      "id": "w8LXL6D6826U",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNSzLkeMAry0"
      },
      "source": [
        "**Scrapping Data Review Teks**"
      ],
      "id": "iNSzLkeMAry0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_QBEMgYJB2S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "171ec027-c94d-461e-bd69-9c50e076b961"
      },
      "source": [
        "Hasil_Scrapping = pd.read_csv('https://raw.githubusercontent.com/sintia123445/REGRESI-LINIER/main/Pilkada_DKI.csv')\n",
        "Hasil_Scrapping.head()                                                   "
      ],
      "id": "x_QBEMgYJB2S",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Pasangan Calon</th>\n",
              "      <th>Text Tweet</th>\n",
              "      <th>content_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>Banyak akun kloning seolah2 pendukung # agussi...</td>\n",
              "      <td>akun kloning dukung agussilvy serang paslon an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>#agussilvy bicara apa kasihan yaa...lap itu ai...</td>\n",
              "      <td>agussilvy bicara kasihan yaa lap air mata wkwk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>Kalau aku sih gak nunggu hasil akhir QC tp lag...</td>\n",
              "      <td>nunggu qc nunggu motif cuit sbyudhoyono pasca ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>Kasian oh kasian dengan peluru 1milyar untuk t...</td>\n",
              "      <td>kasi oh kasi peluru rw agussilvy mempan menang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>negative</td>\n",
              "      <td>Agus-Sylvi</td>\n",
              "      <td>Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...</td>\n",
              "      <td>dukung agussilvy hayo dukung aniessandi putar ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                  content_processed\n",
              "0   1  ...  akun kloning dukung agussilvy serang paslon an...\n",
              "1   2  ...  agussilvy bicara kasihan yaa lap air mata wkwk...\n",
              "2   3  ...  nunggu qc nunggu motif cuit sbyudhoyono pasca ...\n",
              "3   4  ...  kasi oh kasi peluru rw agussilvy mempan menang...\n",
              "4   5  ...  dukung agussilvy hayo dukung aniessandi putar ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pq4dx5pAry5"
      },
      "source": [
        "**Mengambil Series Data Teks Review**"
      ],
      "id": "-pq4dx5pAry5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycKZ9Fd_zVlk",
        "outputId": "2dad130d-31a7-438f-92e9-761f400d9dbe"
      },
      "source": [
        "teks=Hasil_Scrapping['Text Tweet']\n",
        "teks"
      ],
      "id": "ycKZ9Fd_zVlk",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      Banyak akun kloning seolah2 pendukung # agussi...\n",
              "1      #agussilvy bicara apa kasihan yaa...lap itu ai...\n",
              "2      Kalau aku sih gak nunggu hasil akhir QC tp lag...\n",
              "3      Kasian oh kasian dengan peluru 1milyar untuk t...\n",
              "4      Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...\n",
              "                             ...                        \n",
              "895    Kali saja bpk @aniesbaswedan @sandiuno lihat, ...\n",
              "896    Kita harus dapat merangkul semua orang tanpa b...\n",
              "897    Ini jagoanku dibidang digital <Smiling Face Wi...\n",
              "898                 #PesanBijak #OkeOce #GubernurGu3 ...\n",
              "899    Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...\n",
              "Name: Text Tweet, Length: 900, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhcTDo8H21es"
      },
      "source": [
        "## **Teks Preprocessing**"
      ],
      "id": "hhcTDo8H21es"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qz511XJAry7"
      },
      "source": [
        "Setelah mendapat data teks. Salah satu tantangan dari data teks adalah bentuknya yang sangat beragam. Sebuah kata dapat ditulis dengan berbagai bentuk. Kemudian juga besar sekali kemungkinan adalah kesalahan penulisan. Tanda baca, angka, dan lain-lain. Oleh sebab itu, sebelum diolah lebih lanjut untuk diproses menjadi data numerik, maka diperlukan pemrosesan data teks agar menjadi bentuk yang lebih bersih dan standar. Yang akan sangat mempengaruhi hasil analisis data teks tersebut. Pada sentimen analisis misalnya, langkah ini menjadi sangat penting. Ada beberapa hal yang dilakukan pada tahap Teks Preprocessing:"
      ],
      "id": "7Qz511XJAry7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WGQQ4T98_XI"
      },
      "source": [
        "#### 1. **Case Folding & Data Cleaning**"
      ],
      "id": "3WGQQ4T98_XI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXYn3PQR8_XJ"
      },
      "source": [
        "Case folding adalah salah satu bentuk text preprocessing yang paling sederhana dan efektif meskipun sering diabaikan. Tujuan dari case folding untuk mengubah semua huruf dalam dokumen menjadi huruf kecil. Hanya huruf ‘a’ sampai ‘z’ yang diterima. Karakter selain huruf dihilangkan dan dianggap delimiter. \n",
        "\n",
        "Ada beberapa cara yang dapat digunakan dalam tahap case folding, diantaranya:\n",
        "* Menghapus tanda baca\n",
        "* Menghapus angka\n",
        "* Mengubah text menjadi lowercase\n",
        "* Menghapus whitepace (karakter kosong)\n"
      ],
      "id": "EXYn3PQR8_XJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "sJggXB-tNSGY",
        "outputId": "6be48c3e-d99b-425f-fec8-50205349c61f"
      },
      "source": [
        "# Menghapus tanda baca\n",
        "print(teks[10])\n",
        "teks[10]=re.sub(r'[^\\w]|_',' ', teks[10])\n",
        "teks[10]"
      ],
      "id": "sJggXB-tNSGY",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dammnn    politik cantik  SBY ngorbanin  AHY\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dammnn    politik cantik  SBY ngorbanin  AHY'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "K5FjOAuUN2LM",
        "outputId": "d77be66b-2d17-4203-c31e-b62ebb669aa0"
      },
      "source": [
        "# Menghapus angka\n",
        "print(teks[150])\n",
        "teks[150] = re.sub(\"\\S*\\d\\S*\", \"\", teks[150]).strip()\n",
        "teks[150] = re.sub(r\"\\b\\d+\\b\", \" \", teks[150])\n",
        "teks[150]"
      ],
      "id": "K5FjOAuUN2LM",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doa ku.. Semoga suaranya mas Agus-Sylvi beneran ke #Badja  yakin kan mas Agus.. Aku pada mu..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Doa ku.. Semoga suaranya mas Agus-Sylvi beneran ke #Badja  yakin kan mas Agus.. Aku pada mu..'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "gpbLVAk7MJhw",
        "outputId": "7c4dff76-b724-42d5-c436-5f564a1f7a1a"
      },
      "source": [
        "#Mengubah text menjadi lowercase\n",
        "print(teks[25])\n",
        "\n",
        "teks[25]=teks[25].lower()\n",
        "teks[25]"
      ],
      "id": "gpbLVAk7MJhw",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jangan2 rawan gembos ahy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jangan2 rawan gembos ahy'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "rIRfO9AvOIZm",
        "outputId": "efc75755-ad09-4835-8603-de1a17938141"
      },
      "source": [
        "# Menghapus white space\n",
        "print(teks[32])\n",
        "\n",
        "teks[32]=re.sub('[\\s]+', ' ', teks[32])\n",
        "teks[32]"
      ],
      "id": "rIRfO9AvOIZm",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antasari Diancam Harry Tanoe https://youtu.be/9KbbpAjibTc #Antasari #ahok #Sby #AHY #berita #MetroTV #Indonesia #JumatBerkah #KimJongNam\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Antasari Diancam Harry Tanoe https://youtu.be/9KbbpAjibTc #Antasari #ahok #Sby #AHY #berita #MetroTV #Indonesia #JumatBerkah #KimJongNam'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Fhw3WCw3UQ"
      },
      "source": [
        "Membuat Fungsi untuk Melakukan Case Folding"
      ],
      "id": "K-Fhw3WCw3UQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LVwHNFjszPX"
      },
      "source": [
        "import re, string, unicodedata \n",
        "def Case_Folding(text):\n",
        "    # Hapus non-ascii\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    \n",
        "    # Menghapus Tanda Baca\n",
        "    text = re.sub(r'[^\\w]|_',' ', text)\n",
        "    \n",
        "    # Menghapus Angka\n",
        "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
        "    text = re.sub(r\"\\b\\d+\\b\", \" \", text)\n",
        "    \n",
        "    # Mengubah text menjadi lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Menghapus white space\n",
        "    text = re.sub('[\\s]+', ' ', text)\n",
        "    \n",
        "    return text"
      ],
      "id": "7LVwHNFjszPX",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hKe7Zdmyhkz"
      },
      "source": [
        "## **Lemmatization**\n",
        "\n",
        "Proses pengurangan berbagai bentuk kata yang berubah menjadi satu bentuk untuk memudahkan analisis. e.g. kata dari “swim”, “swimming”, “swims”, “swam”, adalah semua bentuk dari “swim”. Nah jadi lemma dari semua kata-kata tersebut adalah “swim”.\n",
        "\n",
        "Untuk data teks berbahasa Indonesia, kita akan menggunakan library `nlp-id`. Pertama kita harus menginstallnya terlebih dahulu."
      ],
      "id": "1hKe7Zdmyhkz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q7a3MxJ-zEK",
        "outputId": "cbb134bc-9e73-4a21-d6ca-38ea1fa400b1"
      },
      "source": [
        "!pip install nlp-id  "
      ],
      "id": "1q7a3MxJ-zEK",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlp-id\n",
            "  Downloading nlp_id-0.1.12.0.tar.gz (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 6.1 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22\n",
            "  Downloading scikit_learn-0.22-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 46.2 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->nlp-id) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp-id) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp-id) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp-id) (1.4.1)\n",
            "Building wheels for collected packages: nlp-id, nltk, wget\n",
            "  Building wheel for nlp-id (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlp-id: filename=nlp_id-0.1.12.0-py3-none-any.whl size=8074105 sha256=590ccc674bd8ece8991618271d6d9fdb0d79ad7aa52e91b1b59a802055d21677\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/50/48/da59531125bd94f48dfe66140f41d8fd8a4f04062050375013\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449924 sha256=eb433143ef4526c9b030c6a038603eb0c63121ff98bcd0e718a8aeaf88a023f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=51f0ddc70f9036af90a3d5ce3a4278fdcae16bc18cd858407adbb0e1ab979fbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built nlp-id nltk wget\n",
            "Installing collected packages: wget, scikit-learn, nltk, nlp-id\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nlp-id-0.1.12.0 nltk-3.4.5 scikit-learn-0.22 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEq7gZEaArzS"
      },
      "source": [
        "Kemudian kita akan menggunakan fungsi `Lemmatizer()` untuk melakukan lemmatisasi data teks."
      ],
      "id": "JEq7gZEaArzS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "4WiD2Sl6PifY",
        "outputId": "312b5fd0-3166-4db1-b720-59c5fc273118"
      },
      "source": [
        "#Lemmatization\n",
        "from nlp_id.lemmatizer import Lemmatizer\n",
        "lemmatizer = Lemmatizer()\n",
        "print(teks[25])\n",
        "teks[25]=lemmatizer.lemmatize(teks[25])\n",
        "teks[25]"
      ],
      "id": "4WiD2Sl6PifY",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jangan2 rawan gembos ahy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jangan2 rawan gembos ahy'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZuxFiwR8_XN"
      },
      "source": [
        "## **Stemming**\n",
        "\n",
        "Stemming merupakan suatu proses untuk menemukan kata dasar dari sebuah kata. Dengan menghilangkan semua imbuhan (affixes) baik yang terdiri dari awalan (prefixes), sisipan (infixes), akhiran (suffixes) dan confixes (kombinasi dari awalan dan akhiran) pada kata turunan. Stemming digunakan untuk mengganti bentuk dari suatu kata menjadi kata dasar dari kata tersebut yang sesuai dengan struktur morfologi Bahasa Indonesia yang baik dan benar. \n",
        "\n",
        "Untuk data teks berbahasa Indonesia, kita akan menggunakan library `PySastrawi`. Pertama kita harus menginstallnya terlebih dahulu."
      ],
      "id": "aZuxFiwR8_XN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5mqgxI4_0XP",
        "outputId": "a90ef2bc-1984-4294-89f9-684b6572af19"
      },
      "source": [
        "!pip install PySastrawi"
      ],
      "id": "e5mqgxI4_0XP",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PySastrawi\n",
            "  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 174 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 194 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 204 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 210 kB 7.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: PySastrawi\n",
            "Successfully installed PySastrawi-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMR8tYBLArzU"
      },
      "source": [
        "Kemudian kita akan menggunakan fungsi `StemmerFactory()` untuk melakukan stemming."
      ],
      "id": "CMR8tYBLArzU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "QzbAftDzyXzr",
        "outputId": "f7f5fbfc-e6c9-40e8-d0e2-f7a65ddf85d1"
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "# Membuat stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "print(teks[25])\n",
        "teks[25] = stemmer.stem(teks[25])\n",
        "teks[25]"
      ],
      "id": "QzbAftDzyXzr",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jangan2 rawan gembos ahy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jangan2 rawan gembos ahy'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zON48uZVs6_o"
      },
      "source": [
        "## **Slang Words**\n",
        "\n",
        "Slang adalah kata-kata yang tidak baku secara bahasa namun sering dipakai oleh pengguna bahasa. Kita perlu melakukan standarisasi untuk slang.\n"
      ],
      "id": "zON48uZVs6_o"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbyY7GHw0j9r"
      },
      "source": [
        "slang_dictionary = pd.read_csv('https://raw.githubusercontent.com/sintia123445/REGRESI-LINIER/main/Pilkada_DKI.csv')\n",
        "slang_dict = pd.Series(slang_dictionary['Sentiment'].values,index=slang_dictionary['Text Tweet'])"
      ],
      "id": "gbyY7GHw0j9r",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOgTQRMd1H_x"
      },
      "source": [
        "def Slangwords(text):\n",
        "    for word in text.split():\n",
        "        if word in slang_dict.keys():\n",
        "            text = text.replace(word, slang_dict[word])\n",
        "    return text"
      ],
      "id": "ZOgTQRMd1H_x",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "_TsMUd5M1Ra0",
        "outputId": "3c153673-3fcc-4475-9528-d3494d5021e6"
      },
      "source": [
        "print(teks[0])\n",
        "\n",
        "teks[0]=Slangwords(teks[0]) \n",
        "teks[0]"
      ],
      "id": "_TsMUd5M1Ra0",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Banyak akun kloning seolah2 pendukung # agussilvy mulai menyerang paslon # aniessandi dengan opini dan argumen pmbenaran..jangan terkecoh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Banyak akun kloning seolah2 pendukung # agussilvy mulai menyerang paslon # aniessandi dengan opini dan argumen pmbenaran..jangan terkecoh'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVGt577V8_XS"
      },
      "source": [
        "## **Stopword**"
      ],
      "id": "gVGt577V8_XS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DsIKA3L8_XS"
      },
      "source": [
        "Stop words adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna.  Stop words umumnya dimanfaatkan dalam task information retrieval, termasuk oleh Google (penjelasannya di sini).  Contoh stop words untuk bahasa Inggris diantaranya “of”, “the”.  Sedangkan untuk bahasa Indonesia diantaranya “yang”, “di”, “ke”."
      ],
      "id": "3DsIKA3L8_XS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUkzRYcw8_XT"
      },
      "source": [
        "from nlp_id.stopword import StopWord\n",
        "stopword = StopWord()"
      ],
      "id": "NUkzRYcw8_XT",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "VLuAcfbsQApk",
        "outputId": "db633cc8-df17-4737-de3d-7f7e83f7a598"
      },
      "source": [
        "from nlp_id.stopword import StopWord\n",
        "print(teks[25])\n",
        "\n",
        "teks[25]=stopword.remove_stopword(teks[25])\n",
        "teks[25]"
      ],
      "id": "VLuAcfbsQApk",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jangan2 rawan gembos ahy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jangan2 rawan gembos ahy'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAlskIRe1eJW"
      },
      "source": [
        "## **Unwanted Words**"
      ],
      "id": "CAlskIRe1eJW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA7PXfhgArzY"
      },
      "source": [
        "Unwanted words adalah kata-kata yang berada di luar beberapa hal di atas namun perlu untuk kita hapus. Kita bisa mendefinisikan sendiri kata-kata atau karakter yang ingin kita hilangkan dari data teks yang kita peroleh."
      ],
      "id": "BA7PXfhgArzY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaxO9CLL1lk2"
      },
      "source": [
        "unwanted_words = ['sy', 'karna', 'gue', 'pun', 'nya', 'yg', 'gw', 'ke', 'gak', 'ga', 'buat', 'selama', 'akan', 'gua', 'gw', \n",
        "                 'gue', 'dampak', 'tau', 'banget', 'mohon', 'dii', 'kalo', 'dll', 'kadang', 'ya', 'coba', 'langsung',\n",
        "                 'cuman', 'cuma', 'biar', 'an', 'kayak', 'dar', 'bikin', 'ssaja', 'sih', 'si', 'situ', 'e', 'utk', 'pake',\n",
        "                 'diin', 'serba', 'ampun', 'untuj', 'deh', 'jd', 'ku', 'total', 'lg', 'arti', 'terimakasih','and', 'udah',\n",
        "                 'kali', 'dasar', 'tiada', 'indonesia', 'pas', 'tidiak', 'the', 'http', 'co', 'com', 'di', 'https', 'kak',\n",
        "                 'dr', 'aja', 'klo', 'tp', 'gitu', 'udh', 'min', 'halo', 'tidak', 'bisa', 'sudah', 'yg', 'apa', 'malah',\n",
        "                 'masih', 'mau', 'kok', 'belum', 'buat', 'atau', 'sama', 'gak', 'ga', 'udah', 'banyak', 'selalu', 'masuk',\n",
        "                 'atau', 'belum', 'ini', 'tp', 'ke', 'ya', 'itu', 'aja', 'saja', 'juga', 'aplikasi', 'my pertamina',\n",
        "                 'mypertamina', 'maaf', 'gk', 'tdk', 'trus', 'jg', 'nih', 'sdh', 'mulu', 'padahal', 'kenapa', 'gimana',\n",
        "                 'gmn', 'sih', 'bs', 'suruh', 'tolong', 'dah', 'bagus', 'my', 'pertamina', 'jadi', 'kalau', 'engenggakk',\n",
        "                 'engenggak', 'pakai', 'bilang', 'mending', 'hasil', 'orang', 'muncul', 'ssudah', 'kasih', 'mala', 'malah']"
      ],
      "id": "eaxO9CLL1lk2",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHE5qBrT19iT",
        "outputId": "4589f577-4a1d-4d6f-e2dc-bbde425e6129"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def RemoveUnwantedwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [word for word in word_tokens if not word in unwanted_words]\n",
        "    return ' '.join(filtered_sentence)"
      ],
      "id": "PHE5qBrT19iT",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "fb2bfQjr2H_F",
        "outputId": "19c240f4-7cb2-4ca9-e851-ee797fbf6339"
      },
      "source": [
        "print(teks[0])\n",
        "\n",
        "teks[0]=RemoveUnwantedwords(teks[0])\n",
        "teks[0]"
      ],
      "id": "fb2bfQjr2H_F",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Banyak akun kloning seolah2 pendukung # agussilvy mulai menyerang paslon # aniessandi dengan opini dan argumen pmbenaran..jangan terkecoh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Banyak akun kloning seolah2 pendukung # agussilvy mulai menyerang paslon # aniessandi dengan opini dan argumen pmbenaran..jangan terkecoh'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLeMXFq25Z7z"
      },
      "source": [
        "## **Menerapkan Semua Langkah**"
      ],
      "id": "rLeMXFq25Z7z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jxqjzry5fHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb522b4d-3ada-42b3-cb20-58784d3770d5"
      },
      "source": [
        "Hasil_Scrapping['content_processed'] = ''\n",
        "\n",
        "for i, row in Hasil_Scrapping.iterrows():\n",
        "    content = Hasil_Scrapping['Text Tweet'][i]\n",
        "    result = Case_Folding(content)\n",
        "    result = lemmatizer.lemmatize(result)\n",
        "    result = stemmer.stem(result)\n",
        "    result = Slangwords(result)\n",
        "    result = stopword.remove_stopword(result)\n",
        "    result = RemoveUnwantedwords(result)\n",
        "    Hasil_Scrapping['content_processed'][i] = result"
      ],
      "id": "6jxqjzry5fHo",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFkPGs4BArzb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "b0a9c077-8742-481f-fcd8-fc0f5790ab1c"
      },
      "source": [
        "Hasil_Scrapping[['Text Tweet', 'content_processed']]"
      ],
      "id": "hFkPGs4BArzb",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text Tweet</th>\n",
              "      <th>content_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Banyak akun kloning seolah2 pendukung # agussi...</td>\n",
              "      <td>akun kloning dukung agussilvy serang paslon an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#agussilvy bicara apa kasihan yaa...lap itu ai...</td>\n",
              "      <td>agussilvy bicara kasihan yaa lap air mata wkwk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kalau aku sih gak nunggu hasil akhir QC tp lag...</td>\n",
              "      <td>nunggu qc nunggu motif cuit sbyudhoyono pasca ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kasian oh kasian dengan peluru 1milyar untuk t...</td>\n",
              "      <td>kasi oh kasi peluru rw agussilvy mempan menang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...</td>\n",
              "      <td>dukung agussilvy hayo dukung aniessandi putar ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>Kali saja bpk @aniesbaswedan @sandiuno lihat, ...</td>\n",
              "      <td>bpk aniesbaswedan sandiuno lihat rspun selfie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>Kita harus dapat merangkul semua orang tanpa b...</td>\n",
              "      <td>rangkul batas usia kelamin okeoce ok hand sala...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>Ini jagoanku dibidang digital &lt;Smiling Face Wi...</td>\n",
              "      <td>jago bidang digital smiling face with sunglass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>#PesanBijak #OkeOce #GubernurGu3 ...</td>\n",
              "      <td>pesanbijak okeoce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...</td>\n",
              "      <td>sandiaga bangun rumah dp simpel banding tol ci...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Text Tweet                                  content_processed\n",
              "0    Banyak akun kloning seolah2 pendukung # agussi...  akun kloning dukung agussilvy serang paslon an...\n",
              "1    #agussilvy bicara apa kasihan yaa...lap itu ai...  agussilvy bicara kasihan yaa lap air mata wkwk...\n",
              "2    Kalau aku sih gak nunggu hasil akhir QC tp lag...  nunggu qc nunggu motif cuit sbyudhoyono pasca ...\n",
              "3    Kasian oh kasian dengan peluru 1milyar untuk t...  kasi oh kasi peluru rw agussilvy mempan menang...\n",
              "4    Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...  dukung agussilvy hayo dukung aniessandi putar ...\n",
              "..                                                 ...                                                ...\n",
              "895  Kali saja bpk @aniesbaswedan @sandiuno lihat, ...  bpk aniesbaswedan sandiuno lihat rspun selfie ...\n",
              "896  Kita harus dapat merangkul semua orang tanpa b...  rangkul batas usia kelamin okeoce ok hand sala...\n",
              "897  Ini jagoanku dibidang digital <Smiling Face Wi...  jago bidang digital smiling face with sunglass...\n",
              "898               #PesanBijak #OkeOce #GubernurGu3 ...                                  pesanbijak okeoce\n",
              "899  Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...  sandiaga bangun rumah dp simpel banding tol ci...\n",
              "\n",
              "[900 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JfNyKZOBDRt"
      },
      "source": [
        "Hasil_Scrapping.to_csv('Pilkada_DKI.csv',index=False)"
      ],
      "id": "6JfNyKZOBDRt",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EFOZ2iNBG4n",
        "outputId": "a7f8ef70-6f16-426e-a1d8-e6078819950a"
      },
      "source": [
        "Hasil_Scrapping.info()"
      ],
      "id": "-EFOZ2iNBG4n",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 900 entries, 0 to 899\n",
            "Data columns (total 5 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Id                 900 non-null    int64 \n",
            " 1   Sentiment          900 non-null    object\n",
            " 2   Pasangan Calon     900 non-null    object\n",
            " 3   Text Tweet         900 non-null    object\n",
            " 4   content_processed  900 non-null    object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 35.3+ KB\n"
          ]
        }
      ]
    }
  ]
}